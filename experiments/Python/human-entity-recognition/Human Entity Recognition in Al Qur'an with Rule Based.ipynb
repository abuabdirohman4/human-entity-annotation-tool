{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Entity in Al Qur'an with NER & Rule Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Buckwalter\n",
    "Mengubah form latin dalam dataset qorpus menjadi arab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install module\n",
    "# pip install lang-trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from lang_trans.arabic import buckwalter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "quranicDF = pd.read_excel('./Dataset/quranic-corpus-morphology-0.4.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 row data\n",
    "quranicDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First Buckwaltering\n",
    "arab = []\n",
    "\n",
    "for i in range (len(quranicDF)):\n",
    "    if (pd.isna(quranicDF.iloc[i,0])):\n",
    "        arab.append(\"\")\n",
    "    else:\n",
    "        arab.append(buckwalter.untransliterate(quranicDF.iloc[i,0]))\n",
    "    (str(i) + ' ' +arab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Bucwaltering\n",
    "Menghilangkan 3 char yang aneh dan mereplace 1 char lalu bucwaltering semuanya ke arab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package regular expression\n",
    "import re\n",
    "\n",
    "arab = []\n",
    "\n",
    "for i in range (len(quranicDF)):\n",
    "    \n",
    "    if (pd.isna(quranicDF.iloc[i,0])):\n",
    "        arab.append(\"\")\n",
    "    else:\n",
    "    \n",
    "        quranicDF.iloc[i,0] = re.sub(r'^aA', 'A`', quranicDF.iloc[i,0]) #belum paham parameter string/df.iloc[i,0] #Done\n",
    "\n",
    "        if quranicDF.iloc[i,0] == 'a':\n",
    "            arab.append(quranicDF.iloc[i,0].replace(r'^a$', '>a')) #belum paham $ nya #Done\n",
    "        else:\n",
    "            arab.append(buckwalter.untransliterate(quranicDF.iloc[i,0]))\n",
    "            \n",
    "        arab[i] = arab[i].replace('^', '')\n",
    "        arab[i] = arab[i].replace('@', '')\n",
    "        arab[i] = arab[i].replace('[', '')\n",
    "    str(i) + ' ' + arab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert data to variable\n",
    "quranicDF['ARAB'] = arab\n",
    "arab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data\n",
    "quranicDF.to_excel(\"./Output/quranic-corpus-morphology-0.4-with-arab.xlsx\")\n",
    "# quranic_df.to_csv(\"./Output/quranic-corpus-morphology-0.4-with-arab.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Human Entity Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "entityDF = pd.read_excel('./Dataset/human-entity-juz-3.xlsx')\n",
    "pd.set_option('display.max_rows', 999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "entityDF = pd.read_excel('./Dataset/human-entity-10-juz.xlsx')\n",
    "pd.set_option('display.max_rows', 999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findHumanEntityPatterns(file):\n",
    "    \n",
    "    tempPatterns = [[]]\n",
    "    finalPatterns = []\n",
    "    currentTag = 0\n",
    "    opentagCount = 0\n",
    "    df = pd.DataFrame({'pattern': []})\n",
    "    \n",
    "    for i in range(len(file)):\n",
    "\n",
    "        closetags = [char for char in str(file.at[i, \"closetag\"])]\n",
    "        opentags = [char for char in str(file.at[i, \"opentag\"])]\n",
    "\n",
    "        for j in range(len(opentags)):\n",
    "            if opentags[j] == '(':\n",
    "                opentagCount += 1\n",
    "\n",
    "        if opentagCount > 0:\n",
    "            tempPatterns[currentTag].append(file.at[i, \"tag\"])\n",
    "\n",
    "        for k in range(len(closetags)):\n",
    "            if closetags[k] == ')':\n",
    "                if opentagCount == 1:\n",
    "                    toInput = (str(tempPatterns[currentTag]))\n",
    "                    df = df.append({'pattern' : toInput}, ignore_index=True)\n",
    "                    finalPatterns.append(tempPatterns[currentTag])\n",
    "                    currentTag += 1\n",
    "                    tempPatterns.append([])\n",
    "                opentagCount -= 1\n",
    "                \n",
    "    if tempPatterns[len(tempPatterns)-1] == []:\n",
    "        del tempPatterns[len(tempPatterns)-1]\n",
    "\n",
    "    # Ini apaan coba\n",
    "    tempPatterns = set(tuple(element) for element in tempPatterns)\n",
    "    tempPatterns = [list(t) for t in set(tuple(element) for element in tempPatterns)]    \n",
    "        \n",
    "#     return tempPatterns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = findHumanEntityPatterns(entityDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use return df for good looking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(patterns))\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = patterns.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "print(len(patterns))\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Human Entity Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "quranicArab = pd.read_excel('./Output/quranic-corpus-morphology-0.4-with-arab.xlsx', encoding='utf8')\n",
    "pd.set_option('display.max_rows', 999999) # Untuk menampilkan semua data sekaligus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def convertAndGroupToArray(data):\n",
    "    \n",
    "    ayats = [[]]\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        location = row['LOCATION']\n",
    "        ayat = location.split(':')[1]\n",
    "        data.at[index, 'LOCATION'] = ayat\n",
    "\n",
    "    length = len(data)\n",
    "    i = 0\n",
    "    data.at[0, 'LOCATION'] = 0\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        # tokenizing location\n",
    "        if index < length-1:\n",
    "            currentLocation = row['LOCATION']\n",
    "            nextLocation = data.at[index+1, 'LOCATION']\n",
    "\n",
    "            if index == length-2 or currentLocation != nextLocation:\n",
    "                i = i+1\n",
    "                ayats.append([])\n",
    "            \n",
    "            data.at[index, 'LOCATION'] = i\n",
    "            \n",
    "            words = {\n",
    "                'OPEN TAG' : '',\n",
    "                'AYAT' : int(row['LOCATION']),\n",
    "                'ARAB' : row['ARAB'],\n",
    "                'TAG' : row['TAG'],\n",
    "                'CLOSE TAG' : ''\n",
    "            }\n",
    "            \n",
    "            ayats[i].append(words)\n",
    "    \n",
    "    return ayats\n",
    "     \n",
    "ayats = convertAndGroupToArray(quranicArab[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule based logic\n",
    "def findHumanEntity(ayats):\n",
    "    \n",
    "    ayatIndex = 0\n",
    "    entityIndex = 0\n",
    "    for ayat in ayats:\n",
    "        for pattern in patterns:\n",
    "            patternLength = len(pattern)\n",
    "            for wordIndex in range(len(ayat) - patternLength):\n",
    "                # Get set of words based on the pattern length\n",
    "                wordTagsSet = []\n",
    "                for wordOfSetIndex in range((patternLength)):\n",
    "                    wordTagsSet.append(ayat[wordIndex+wordOfSetIndex]['TAG'])\n",
    "                if str(wordTagsSet) == str(pattern):\n",
    "                    ayat[wordIndex]['OPEN TAG'] += str(entityIndex) + '('\n",
    "                    ayat[wordIndex+patternLength]['CLOSE TAG'] += str(entityIndex) + ')'\n",
    "                    entityIndex += 1\n",
    "        ayatIndex += 1\n",
    "                \n",
    "findHumanEntity(ayats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ayatDF = pd.DataFrame({'AYAT': [], 'OPEN TAG': [], 'ARAB': [], 'TAG': [], 'CLOSE TAG': []});\n",
    "\n",
    "ayatCollections = []\n",
    "openTagCollections = []\n",
    "arabCollections = []\n",
    "tagCollections = []\n",
    "closeTagCollections = []\n",
    "\n",
    "for ayat in ayats:\n",
    "    for word in ayat:\n",
    "        ayatDF = ayatDF.append({'AYAT': word['AYAT'], 'OPEN TAG': word['OPEN TAG'], 'ARAB': word['ARAB'], 'TAG': word['TAG'], 'CLOSE TAG': word['CLOSE TAG']}, ignore_index=True)\n",
    "        \n",
    "ayatDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Placing Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataForEntity = pd.read_excel('./Output/quranic-corpus-morphology-0.4-with-arab.xlsx')\n",
    "dataOfStandardizationResource = pd.read_excel('./Dataset/wordbywordFromCorpus.xlsx')\n",
    "\n",
    "def strip_arabic_vowels(line_with_vowels):\n",
    "    diacritics = [u'\\u064e',  # fatha, short a\n",
    "                  u'\\u064b',  # double fatha\n",
    "                  u'\\u0650',  # kasra, short i\n",
    "                  u'\\u064d',  # double kasra\n",
    "                  u'\\u064f',  # damma, short u\n",
    "                  u'\\u064c',  # double damma\n",
    "                  u'\\u0652',  # sukkun, nothing\n",
    "                  u'\\u0651',  # shedda, double\n",
    "                  u'\\u0670',\n",
    "                  u'\\u0671',\n",
    "                  u'\\u0653',\n",
    "                  u'\\u08EA',\n",
    "                  u'\\uFBB2',\n",
    "                  u'\\u0660',\n",
    "                  u'\\u06DF',\n",
    "                  u'\\u06E2',\n",
    "                  u'\\u06E5',\n",
    "                  ',',\n",
    "                  '.',\n",
    "                  u'\\u06E7',\n",
    "                  u'\\u06E6',\n",
    "                  u'\\u0654',\n",
    "                  u'\\u06ED',\n",
    "                  '#',\n",
    "                  '[',\n",
    "                  ']',\n",
    "                  ':',\n",
    "                  u'\\u06DC',\n",
    "                  '\"',\n",
    "                  u'\\u06e0',\n",
    "                  u'\\u06e8'\n",
    "                 ]\n",
    "    \n",
    "    line_without_vowels = ''\n",
    "    for char in line_with_vowels:\n",
    "        if char not in diacritics:\n",
    "            line_without_vowels += char\n",
    "    \n",
    "    return line_without_vowels\n",
    "    \n",
    "dosrIndex = 0\n",
    "\n",
    "import re\n",
    "\n",
    "for i in range(len(dataForEntity)):\n",
    "    query = strip_arabic_vowels(str(dataForEntity.at[i, 'ARAB']))\n",
    "    nextQuery = strip_arabic_vowels(str(dataForEntity.at[i+1, 'ARAB']))\n",
    "    source = strip_arabic_vowels(str(dataOfStandardizationResource.at[dosrIndex, 'Data']))\n",
    "    \n",
    "    print(str(i) + ' | ' + query + ' | ' + source)\n",
    "    \n",
    "    exist = re.search(query + \"$\", source)\n",
    "    queryLocation = -1\n",
    "    \n",
    "    nextQueryExist = re.search(nextQuery + \"$\", source)\n",
    "\n",
    "    if exist is not None:\n",
    "        queryLocation = exist.span()[0]\n",
    "    \n",
    "    if nextQueryExist is not None:\n",
    "        if nextQueryExist.span()[0] == 0:\n",
    "            nextQueryExist = False\n",
    "            \n",
    "    queryShouldLastIndex = len(source) - len(query)\n",
    "    \n",
    "    if exist and queryShouldLastIndex == queryLocation:\n",
    "        dataForEntity.at[i, 'SPACE AFTER'] = 'TRUE'\n",
    "        print(\"Last\")\n",
    "        dosrIndex += 1\n",
    "    else:\n",
    "        print(\"Not last\") \n",
    "            \n",
    "# pd.set_option(\"display.max_rows\", 99999999)\n",
    "dataForEntity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "berhenti di 74359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataForEntity.to_excel(\"./Output/quranic-corpus-morphology-0.4-with-arab-and-space.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Insert Dataset & Rule to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import pymongo\n",
    "\n",
    "# connecting to mongo\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# create database\n",
    "mydb = myclient[\"TA\"]\n",
    "\n",
    "# create collection\n",
    "quran = mydb[\"quran\"]\n",
    "\n",
    "# import dataset\n",
    "data = pd.read_excel(\"./Output/quranic-corpus-morphology-0.4-with-arab-and-space.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split location dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    location = data.at[i, \"LOCATION\"]\n",
    "    location = location.split(':')\n",
    "    no_surat = location[0][1:] # belum paham\n",
    "    ayat = location[1]\n",
    "    word = location[2]\n",
    "    \n",
    "    data.at[i, 'SURAH_NUMBER'] = no_surat\n",
    "    data.at[i, 'AYAT_NUMBER'] = ayat\n",
    "    data.at[i, 'WORD_NUMBER'] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek data\n",
    "data.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert to mongo\n",
    "quran.insert_many(data.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = quran.find({\"SURAH_NUMBER\": \"67\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns = [['REL', 'V'], ['PN', 'N', 'PN'], ['REL', 'P', 'N', 'PRON'], ['REL', 'V', 'PRON'], ['N', 'PRON'], ['N'], ['COND', 'V', 'P', 'DET', 'N', 'CONJ', 'V', 'P', 'PN'], ['REL', 'V', 'PN'], ['PN'], ['N', 'DET', 'ADJ'], ['REL', 'V', 'P', 'N'], ['REL', 'V', 'PRON', 'N', 'PRON', 'P', 'N', 'PN'], ['REL', 'V', 'N', 'PRON'], ['ADJ'], ['REL', 'V', 'PRON', 'N', 'PRON', 'N', 'N', 'PN'], ['COND', 'V', 'DET', 'N'], ['N', 'DET', 'N'], ['DET', 'N'], ['REL', 'V', 'PRON', 'P', 'N', 'PN'], ['REL', 'V', 'PRON', 'N', 'PRON', 'P', 'DET', 'N', 'CONJ', 'DET', 'N'], ['REL', 'V', 'PRON', 'DET', 'N'], ['COND', 'V', 'PRON'], ['COND', 'V'], ['N', 'DET', 'N', 'PRON'], ['REL', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'DET', 'N', 'CONJ', 'V', 'PRON', 'DET', 'N', 'CONJ', 'V', 'PRON', 'DET', 'N'], ['REL', 'P', 'PRON', 'DET', 'N'], ['COND', 'P', 'N', 'PRON', 'N'], ['N', 'PN'], ['N', 'CONJ', 'DET', 'N'], ['REL', 'V', 'PRON', 'N', 'PRON', 'ACC', 'PRON', 'V', 'PRON', 'REM', 'V', 'P', 'PRON', 'N', 'PRON', 'CONJ', 'V', 'PRON', 'N', 'DET', 'N', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'DET', 'N', 'P', 'DET', 'N', 'V', 'PN', 'ACC', 'PRON', 'NEG', 'N', 'EXP', 'PRON', 'CONJ', 'DET', 'N', 'CONJ', 'N', 'DET', 'N', 'N', 'P', 'DET', 'N', 'NEG', 'N', 'EXP', 'PRON', 'DET', 'N', 'DET', 'ADJ', 'ACC', 'DET', 'N', 'LOC', 'PN', 'DET', 'PN', 'REM', 'NEG', 'V', 'REL', 'V', 'PRON', 'DET', 'N', 'RES', 'P', 'N', 'REL', 'V', 'PRON', 'DET', 'N', 'N', 'LOC', 'PRON', 'REM', 'COND', 'V', 'P', 'N', 'PN', 'RSLT', 'ACC', 'PN', 'N', 'DET', 'N', 'REM', 'COND', 'V', 'PRON', 'PRON', 'RSLT', 'V', 'V', 'PRON', 'N', 'PRON', 'P', 'PN', 'CONJ', 'REL', 'V', 'PRON', 'CONJ', 'V', 'P', 'REL', 'V', 'PRON', 'DET', 'N', 'CONJ', 'DET', 'N', 'INTG', 'V', 'PRON', 'REM', 'COND', 'V', 'PRON', 'RSLT', 'CERT', 'V', 'PRON', 'CONJ', 'COND', 'V', 'PRON', 'RSLT', 'ACC', 'PREV', 'P', 'PRON', 'DET', 'N', 'REM', 'PN', 'N', 'P', 'DET', 'N', 'ACC', 'REL', 'V', 'PRON', 'P', 'N', 'PN', 'CONJ', 'V', 'PRON', 'DET', 'N', 'P', 'N', 'N', 'CONJ', 'V', 'PRON', 'REL', 'V', 'PRON', 'P', 'DET', 'N', 'P', 'DET', 'N', 'REM', 'V', 'PRON', 'P', 'N', 'ADJ', 'DEM', 'REL', 'V', 'N', 'PRON', 'P', 'DET', 'N', 'CONJ', 'DET', 'N', 'REM', 'NEG', 'P', 'PRON', 'P', 'N', 'INTG', 'NEG', 'V', 'P', 'REL', 'V', 'PRON', 'N', 'P', 'DET', 'N', 'V', 'PRON', 'P', 'N', 'PN', 'PRP', 'V', 'LOC', 'PRON', 'CONJ', 'V', 'N', 'P', 'PRON', 'CIRC', 'PRON', 'N', 'DEM', 'P', 'ACC', 'PRON', 'V', 'PRON', 'NEG', 'V', 'PRON', 'DET', 'N', 'RES', 'T', 'ADJ', 'REM', 'V', 'PRON', 'P', 'N', 'PRON', 'REL', 'V', 'PRON', 'V', 'PRON', 'REM', 'INTG', 'T', 'V', 'PRON', 'PRON', 'P', 'N', 'NEG', 'N', 'P', 'PRON', 'CONJ', 'V', 'N', 'N', 'REL', 'V', 'CIRC', 'PRON', 'NEG', 'V', 'PRON', 'V', 'PN', 'VOC', 'N', 'DET', 'N', 'V', 'DET', 'N', 'REL', 'V', 'CONJ', 'V', 'DET', 'N', 'P', 'REL', 'V', 'CONJ', 'V', 'REL', 'V', 'CONJ', 'V', 'REL', 'V', 'P', 'N', 'PRON', 'DET', 'N', 'ACC', 'PRON', 'P', 'N', 'N', 'N', 'V', 'DET', 'N', 'P', 'DET', 'N', 'CONJ', 'V', 'DET', 'N', 'P', 'DET', 'N', 'CONJ', 'V', 'DET', 'N', 'P', 'DET', 'N', 'CONJ', 'V', 'DET', 'N', 'P', 'DET', 'N', 'CONJ', 'V', 'REL', 'V', 'P', 'N', 'N', 'PRO', 'V', 'DET', 'N', 'DET', 'N', 'N', 'P', 'N', 'DET', 'N', 'REM', 'COND', 'V', 'DEM', 'RSLT', 'V', 'P', 'PN', 'P', 'N', 'RES', 'SUB', 'V', 'PRON', 'P', 'PRON', 'N', 'REM', 'V', 'PRON', 'PN', 'N', 'PRON', 'REM', 'P', 'PN', 'DET', 'N', 'V', 'COND', 'V', 'PRON', 'REL', 'P', 'N', 'PRON', 'CONJ', 'V', 'PRON', 'PRON', 'V', 'PRON', 'PN', 'REM', 'V', 'REL', 'P', 'DET', 'N', 'CONJ', 'REL', 'P', 'DET', 'N', 'REM', 'PN', 'P', 'N', 'N', 'N', 'T', 'V', 'N', 'N', 'REL', 'V', 'P', 'N', 'N', 'CONJ', 'REL', 'V', 'P', 'N', 'V', 'SUB', 'ACC', 'LOC', 'PRON', 'CONJ', 'LOC', 'PRON', 'N', 'ADJ', 'REM', 'V', 'PRON', 'PN', 'N', 'PRON', 'REM', 'PN', 'N', 'P', 'DET', 'N', 'V', 'COND', 'V', 'PRON', 'V', 'PRON', 'PN', 'RSLT', 'V', 'PRON', 'PRON', 'V', 'PRON', 'PN', 'CONJ', 'V', 'P', 'PRON', 'N', 'PRON', 'REM', 'PN', 'N', 'ADJ', 'V', 'V', 'PRON', 'PN', 'CONJ', 'DET', 'N', 'REM', 'COND', 'V', 'PRON', 'RSLT', 'ACC', 'PN', 'NEG', 'V', 'DET', 'N', 'ACC', 'PN', 'V', 'PN', 'CONJ', 'PN', 'CONJ', 'N', 'PN', 'CONJ', 'N', 'PN', 'P', 'DET', 'N', 'N', 'N', 'PRON', 'P', 'N', 'REM', 'PN', 'N', 'ADJ', 'T', 'V', 'N', 'PN', 'N', 'PRON', 'ACC', 'PRON', 'V', 'PRON', 'P', 'PRON', 'REL', 'P', 'N', 'PRON', 'N', 'REM', 'V', 'P', 'PRON', 'ACC', 'PRON', 'PRON', 'DET', 'N', 'DET', 'ADJ', 'REM', 'T', 'V', 'PRON', 'V', 'N', 'PRON', 'ACC', 'PRON', 'V', 'PRON', 'PRON', 'N', 'REM', 'PN', 'N', 'P', 'REL', 'V', 'REM', 'V', 'DET', 'N', 'P', 'DET', 'N', 'CONJ', 'ACC', 'PRON', 'V', 'PRON', 'PRON', 'PN', 'CONJ', 'ACC', 'PRON', 'V', 'PRON', 'P', 'PRON', 'CONJ', 'N', 'PRON', 'P', 'DET', 'PN', 'DET', 'ADJ', 'REM', 'V', 'PRON', 'N', 'PRON', 'P', 'N', 'ADJ', 'CONJ', 'V', 'PRON', 'N', 'ADJ', 'CONJ', 'V', 'PRON', 'PN', 'T', 'V', 'P', 'PRON', 'PN', 'DET', 'N', 'V', 'LOC', 'PRON', 'N', 'V', 'VOC', 'PN', 'INTG', 'P', 'PRON', 'DEM', 'V', 'PRON', 'P', 'N', 'PN', 'ACC', 'PN', 'V', 'REL', 'V', 'P', 'N', 'N', 'T', 'V', 'PN', 'N', 'PRON', 'V', 'N', 'PRON', 'V', 'P', 'PRON', 'P', 'N', 'PRON', 'N', 'ADJ', 'ACC', 'PRON', 'N', 'DET', 'N', 'REM', 'V', 'PRON', 'DET', 'N', 'CIRC', 'PRON', 'N', 'V', 'P', 'DET', 'N', 'ACC', 'PN', 'V', 'PRON', 'P', 'PN', 'N', 'P', 'N', 'P', 'PN', 'CONJ', 'N', 'CONJ', 'N', 'CONJ', 'N', 'P', 'DET', 'N', 'V', 'N', 'PRON', 'INTG', 'V', 'P', 'PRON', 'N', 'CIRC', 'CERT', 'V', 'PRON', 'DET', 'N', 'CONJ', 'N', 'PRON', 'N', 'V', 'P', 'DEM', 'PN', 'V', 'REL', 'V', 'V', 'N', 'PRON', 'V', 'P', 'PRON', 'N', 'V', 'N', 'PRON', 'SUB', 'NEG', 'V', 'DET', 'N', 'T', 'N', 'EXP', 'N', 'CONJ', 'V', 'N', 'PRON', 'ADJ', 'CONJ', 'V', 'P', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'T', 'V', 'DET', 'N', 'VOC', 'PN', 'ACC', 'PN', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'P', 'N', 'DET', 'N', 'VOC', 'PN', 'V', 'PRON', 'P', 'N', 'PRON', 'CONJ', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'LOC', 'DET', 'N', 'DEM', 'P', 'N', 'DET', 'N', 'V', 'PRON', 'P', 'PRON', 'REM', 'NEG', 'V', 'PRON', 'LOC', 'PRON', 'T', 'V', 'PRON', 'N', 'PRON', 'INTG', 'PRON', 'V', 'PN', 'CONJ', 'NEG', 'V', 'PRON', 'LOC', 'PRON', 'T', 'V', 'PRON', 'T', 'V', 'DET', 'N', 'VOC', 'PN', 'ACC', 'PN', 'V', 'PRON', 'P', 'N', 'P', 'PRON', 'N', 'PRON', 'DET', 'PN', 'PN', 'N', 'PN', 'N', 'P', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'P', 'DET', 'N', 'CONJ', 'V', 'DET', 'N', 'P', 'DET', 'N', 'CONJ', 'N', 'CONJ', 'P', 'DET', 'N', 'V', 'N', 'PRON', 'INTG', 'V', 'P', 'PRON', 'N', 'CIRC', 'NEG', 'V', 'PRON', 'N', 'V', 'P', 'DEM', 'PN', 'V', 'REL', 'V', 'T', 'V', 'N', 'RSLT', 'ACC', 'PREV', 'V', 'P', 'PRON', 'V', 'CONJ', 'V', 'REM', 'V', 'PRON', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'DET', 'PN', 'CONJ', 'DET', 'PN', 'CONJ', 'N', 'P', 'N', 'PN', 'ACC', 'PRON', 'CERT', 'V', 'PRON', 'PRON', 'P', 'N', 'P', 'N', 'PRON', 'ACC', 'PRON', 'V', 'P', 'PRON', 'P', 'DET', 'N', 'P', 'N', 'DET', 'N', 'CONJ', 'V', 'P', 'PRON', 'CONJ', 'V', 'N', 'P', 'N', 'PN', 'CONJ', 'V', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'V', 'DET', 'N', 'P', 'N', 'PN', 'CONJ', 'V', 'PRON', 'P', 'REL', 'V', 'PRON', 'CONJ', 'REL', 'V', 'PRON', 'P', 'N', 'PRON', 'ACC', 'P', 'DEM', 'EMPH', 'N', 'P', 'PRON', 'COND', 'V', 'PRON', 'N', 'CONJ', 'N', 'P', 'REL', 'LOC', 'N', 'PRON', 'P', 'DET', 'PN', 'CONJ', 'PRP', 'V', 'P', 'PRON', 'N', 'REL', 'V', 'P', 'PRON', 'REM', 'V', 'PRON', 'PRON', 'P', 'N', 'P', 'N', 'PRON', 'REM', 'V', 'PRON', 'PN', 'CONJ', 'V', 'PRON', 'PRON', 'ACC', 'PN', 'N', 'PRON', 'CONJ', 'N', 'PRON', 'REM', 'V', 'PRON', 'PRON', 'DEM', 'N', 'ADJ', 'REM', 'T', 'V', 'PN', 'P', 'PRON', 'DET', 'N', 'V', 'INTG', 'N', 'PRON', 'P', 'PN', 'V', 'DET', 'N', 'PRON', 'N', 'PN', 'V', 'PRON', 'P', 'PN', 'REM', 'V', 'P', 'ACC', 'PRON', 'N', 'N', 'PRON', 'V', 'PRON', 'P', 'REL', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'DET', 'N', 'REM', 'V', 'PRON', 'LOC', 'DET', 'N', 'REM', 'V', 'PRON', 'CONJ', 'V', 'PN', 'REM', 'PN', 'N', 'DET', 'N', 'T', 'V', 'PN', 'VOC', 'PN', 'ACC', 'PRON', 'N', 'PRON', 'CONJ', 'N', 'PRON', 'P', 'PRON', 'CONJ', 'N', 'PRON', 'P', 'REL', 'V', 'PRON', 'CONJ', 'N', 'REL', 'V', 'PRON', 'PRON', 'LOC', 'REL', 'V', 'PRON', 'P', 'N', 'DET', 'N', 'CONJ', 'P', 'PRON', 'N', 'PRON', 'CONJ', 'V', 'LOC', 'PRON', 'P', 'REL', 'V', 'PRON', 'P', 'PRON', 'V', 'PRON', 'REM', 'EXL', 'COND', 'V', 'PRON', 'RSLT', 'V', 'PRON', 'N', 'ADJ', 'P', 'DET', 'N', 'CONJ', 'DET', 'N', 'REM', 'NEG', 'P', 'PRON', 'P', 'N', 'CONJ', 'EXL', 'COND', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'DET', 'N', 'RSLT', 'V', 'PRON', 'N', 'PRON', 'REM', 'PN', 'NEG', 'V', 'DET', 'N', 'DEM', 'V', 'PRON', 'P', 'PRON', 'P', 'DET', 'N', 'CONJ', 'DET', 'N', 'DET', 'ADJ', 'ACC', 'N', 'PN', 'LOC', 'PN', 'P', 'N', 'PN', 'V', 'PRON', 'P', 'N', 'CONJ', 'V', 'P', 'PRON', 'V', 'REM', 'V', 'DET', 'N', 'P', 'N', 'PRON', 'REM', 'PRO', 'V', 'P', 'DET', 'N', 'REM', 'COND', 'V', 'PRON', 'P', 'PRON', 'P', 'N', 'REL', 'V', 'PRON', 'P', 'DET', 'N', 'RSLT', 'V', 'V', 'PRON', 'V', 'N', 'PRON', 'CONJ', 'N', 'PRON', 'CONJ', 'N', 'PRON', 'CONJ', 'N', 'PRON', 'CONJ', 'N', 'PRON', 'CONJ', 'N', 'PRON', 'CONJ', 'V', 'CONJ', 'V', 'N', 'PN', 'P', 'DET', 'N', 'ACC', 'DEM', 'EMPH', 'PRON', 'DET', 'N', 'DET', 'ADJ', 'REM', 'NEG', 'P', 'N', 'RES', 'PN', 'REM', 'ACC', 'PN', 'EMPH', 'PRON', 'DET', 'N', 'DET', 'ADJ', 'REM', 'COND', 'V', 'PRON', 'RSLT', 'ACC', 'PN', 'N', 'P', 'DET', 'N', 'V', 'VOC', 'N', 'DET', 'N', 'V', 'PRON', 'P', 'N', 'ADJ', 'LOC', 'PRON', 'CONJ', 'LOC', 'PRON', 'SUB', 'NEG', 'V', 'RES', 'PN', 'CONJ', 'NEG', 'V', 'P', 'PRON', 'N', 'CONJ', 'NEG', 'V', 'N', 'PRON', 'N', 'N', 'P', 'N', 'PN', 'REM', 'COND', 'V', 'PRON', 'RSLT', 'V', 'PRON', 'V', 'PRON', 'P', 'ACC', 'PRON', 'N', 'VOC', 'N', 'DET', 'N', 'P', 'INTG', 'V', 'PRON', 'P', 'PN', 'CIRC', 'NEG', 'V', 'DET', 'PN', 'CONJ', 'DET', 'PN', 'RES', 'P', 'N', 'PRON', 'INTG', 'SUP', 'NEG', 'V', 'PRON', 'VOC', 'PRON', 'DEM', 'V', 'PRON', 'P', 'REL', 'P', 'PRON', 'P', 'PRON', 'N', 'REM', 'P', 'INTG', 'V', 'PRON', 'P', 'REL', 'V', 'P', 'PRON', 'P', 'PRON', 'N', 'REM', 'PN', 'V', 'CONJ', 'PRON', 'NEG', 'V', 'PRON', 'NEG', 'V', 'PN', 'PN', 'CONJ', 'NEG', 'PN', 'SUP', 'AMD', 'V', 'N', 'ADJ', 'CONJ', 'NEG', 'V', 'P', 'DET', 'N', 'ACC', 'N', 'DET', 'N', 'P', 'PN', 'EMPH', 'REL', 'V', 'PRON', 'PRON', 'CONJ', 'DEM', 'DET', 'N', 'CONJ', 'REL', 'V', 'PRON', 'REM', 'PN', 'N', 'DET', 'N', 'V', 'N', 'P', 'N', 'DET', 'N', 'SUB', 'V', 'PRON', 'PRON', 'REM', 'NEG', 'V', 'PRON', 'RES', 'N', 'PRON', 'CIRC', 'NEG', 'V', 'PRON', 'VOC', 'N', 'DET', 'N', 'P', 'INTG', 'V', 'PRON', 'P', 'N', 'PN', 'CIRC', 'PRON', 'V', 'PRON', 'VOC', 'N', 'DET', 'N', 'P', 'INTG', 'V', 'PRON', 'DET', 'N', 'P', 'DET', 'N', 'CONJ', 'V', 'PRON', 'DET', 'N', 'CIRC', 'PRON', 'V', 'PRON', 'REM', 'V', 'N', 'P', 'N', 'DET', 'N', 'V', 'PRON', 'P', 'REL', 'V', 'P', 'REL', 'V', 'PRON', 'T', 'DET', 'N', 'CONJ', 'V', 'PRON', 'T', 'PRON', 'ACC', 'PRON', 'V', 'PRON', 'CONJ', 'PRO', 'V', 'PRON', 'EXP', 'P', 'REL', 'V', 'N', 'PRON', 'V', 'ACC', 'DET', 'N', 'N', 'PN', 'SUB', 'V', 'N', 'N', 'REL', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'PRON', 'LOC', 'N', 'PRON', 'V', 'ACC', 'DET', 'N', 'P', 'N', 'PN', 'V', 'PRON', 'REL', 'V', 'REM', 'PN', 'N', 'ADJ', 'V', 'P', 'N', 'PRON', 'REL', 'V', 'REM', 'PN', 'N', 'DET', 'N', 'DET', 'ADJ', 'REM', 'P', 'N', 'DET', 'N', 'REL', 'COND', 'V', 'PRON', 'P', 'N', 'V', 'PRON', 'P', 'PRON', 'CONJ', 'P', 'PRON', 'REL', 'COND', 'V', 'PRON', 'P', 'N', 'NEG', 'V', 'PRON', 'P', 'PRON', 'RES', 'SUB', 'V', 'PRON', 'P', 'PRON', 'N', 'DEM', 'P', 'ACC', 'PRON', 'V', 'PRON', 'V', 'P', 'PRON', 'P', 'DET', 'N', 'N', 'REM', 'V', 'PRON', 'P', 'PN', 'DET', 'N', 'CIRC', 'PRON', 'V', 'PRON', 'ANS', 'COND', 'V', 'P', 'N', 'PRON', 'CONJ', 'V', 'RSLT', 'ACC', 'PN', 'V', 'DET', 'N', 'ACC', 'REL', 'V', 'PRON', 'P', 'N', 'PN', 'CONJ', 'N', 'PRON', 'N', 'ADJ', 'DEM', 'NEG', 'N', 'P', 'PRON', 'P', 'DET', 'N', 'CONJ', 'NEG', 'V', 'PRON', 'PN', 'CONJ', 'NEG', 'V', 'P', 'PRON', 'T', 'DET', 'N', 'CONJ', 'NEG', 'V', 'PRON', 'CONJ', 'P', 'PRON', 'N', 'ADJ', 'REM', 'ACC', 'P', 'PRON', 'EMPH', 'N', 'V', 'PRON', 'N', 'PRON', 'P', 'DET', 'N', 'PRP', 'V', 'PRON', 'PRON', 'P', 'DET', 'N', 'REM', 'NEG', 'PRON', 'P', 'DET', 'N', 'CONJ', 'V', 'PRON', 'PRON', 'P', 'N', 'PN', 'REM', 'NEG', 'PRON', 'P', 'N', 'PN', 'CONJ', 'V', 'PRON', 'P', 'PN', 'DET', 'N', 'CIRC', 'PRON', 'V', 'PRON', 'NEG', 'V', 'P', 'N', 'SUB', 'V', 'PRON', 'PN', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'DET', 'N', 'CONJ', 'V', 'P', 'DET', 'N', 'V', 'PRON', 'N', 'P', 'PRON', 'P', 'N', 'PN', 'REM', 'AMD', 'V', 'PRON', 'N', 'P', 'SUB', 'V', 'PRON', 'V', 'PRON', 'DET', 'N', 'CONJ', 'P', 'SUB', 'V', 'PRON', 'V', 'PRON', 'CONJ', 'NEG', 'V', 'PRON', 'SUB', 'V', 'PRON', 'DET', 'N', 'CONJ', 'DET', 'N', 'N', 'INTG', 'V', 'PRON', 'P', 'DET', 'N', 'T', 'T', 'PRON', 'N', 'CONJ', 'T', 'V', 'PN', 'N', 'DET', 'N', 'EMPH', 'REL', 'V', 'PRON', 'PRON', 'P', 'N', 'CONJ', 'N', 'CONJ', 'V', 'PRON', 'N', 'ADJ', 'P', 'REL', 'LOC', 'PRON', 'EMPH', 'V', 'EMPH', 'P', 'PRON', 'CONJ', 'EMPH', 'V', 'EMPH', 'PRON', 'V', 'INTG', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'P', 'DEM', 'N', 'PRON', 'V', 'PRON', 'V', 'PRON', 'V', 'REM', 'V', 'PRON', 'REM', 'PRON', 'LOC', 'PRON', 'P', 'DET', 'N', 'REM', 'COND', 'V', 'T', 'DEM', 'RSLT', 'DEM', 'PRON', 'DET', 'N', 'INTG', 'SUP', 'N', 'N', 'PN', 'V', 'PRON', 'CIRC', 'P', 'PRON', 'V', 'REL', 'P', 'DET', 'N', 'CONJ', 'DET', 'N', 'N', 'CONJ', 'N', 'CIRC', 'P', 'PRON', 'V', 'PRON', 'V', 'V', 'PRON', 'P', 'PN', 'CONJ', 'REL', 'V', 'P', 'PRON', 'CONJ', 'REL', 'V', 'P', 'PN', 'CONJ', 'PN', 'CONJ', 'PN', 'CONJ', 'PN', 'CONJ', 'DET', 'N', 'CONJ', 'REL', 'V', 'PN', 'CONJ', 'PN', 'CONJ', 'DET', 'N', 'P', 'N', 'PRON', 'NEG', 'V', 'LOC', 'N', 'P', 'PRON', 'REM', 'PRON', 'P', 'PRON', 'N', 'REM', 'COND', 'V', 'N', 'DET', 'PN', 'N', 'RSLT', 'NEG', 'V', 'P', 'PRON', 'REM', 'PRON', 'P', 'DET', 'N', 'P', 'DET', 'N', 'INTG', 'V', 'PN', 'N', 'V', 'PRON', 'T', 'N', 'PRON', 'CONJ', 'V', 'PRON', 'ACC', 'DET', 'N', 'N', 'CONJ', 'V', 'PRON', 'DET', 'N', 'REM', 'PN', 'NEG', 'V', 'DET', 'N', 'DET', 'ADJ', 'DEM', 'N', 'PRON', 'ACC', 'P', 'PRON', 'N', 'PN', 'CONJ', 'DET', 'N', 'CONJ', 'DET', 'N', 'N', 'N', 'P', 'PRON', 'NEG', 'V', 'P', 'PRON', 'DET', 'N', 'CONJ', 'NEG', 'PRON', 'V', 'PRON', 'EXP', 'REL', 'V', 'PRON', 'P', 'N', 'DEM', 'CONJ', 'V', 'PRON', 'REM', 'ACC', 'PN', 'N', 'ADJ', 'ACC', 'REL', 'V', 'PRON', 'T', 'N', 'PRON', 'CONJ', 'V', 'PRON', 'N', 'NEG', 'V', 'N', 'PRON', 'REM', 'DEM', 'PRON', 'DET', 'N', 'ACC', 'REL', 'V', 'PRON', 'CONJ', 'V', 'PRON', 'CIRC', 'PRON', 'N', 'REM', 'NEG', 'V', 'P', 'N', 'PRON', 'N', 'DET', 'N', 'N', 'CIRC', 'COND', 'V', 'P', 'PRON', 'DEM', 'P', 'PRON', 'N', 'ADJ', 'CONJ', 'NEG', 'P', 'PRON', 'P', 'N']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(patterns)):\n",
    "    patterns[i] = {\"pattern\": patterns[i]}\n",
    "                                      \n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert patterns\n",
    "mydb[\"patterns\"].insert_many(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = quran.find({\"SURAH_NUMBER\": \"67\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove space in patterns string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = list(mydb[\"patterns\"].find())\n",
    "\n",
    "for pattern in patterns:\n",
    "    stripped_spaces = pattern[\"pattern\"].replace(\" \", \"\")\n",
    "    mydb[\"patterns\"].update_one({\"pattern\": pattern[\"pattern\"]}, {\"$set\" : {\"pattern\": stripped_spaces}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Arabic Entity Classifier Using Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def group(ayats):\n",
    "        \n",
    "    length = len(ayats)\n",
    "    i = 0\n",
    "\n",
    "    grouppedAyats = [[]]\n",
    "\n",
    "    index = 0\n",
    "    for row in ayats:\n",
    "\n",
    "        # tokenizing location\n",
    "        if index < length-1:\n",
    "            currentLocation = row[\"AYAT_NUMBER\"]\n",
    "            nextLocation = ayats[index+1]['AYAT_NUMBER']\n",
    "\n",
    "            if index == length-2 or currentLocation != nextLocation:\n",
    "                i = i+1\n",
    "                grouppedAyats.append([])\n",
    "            \n",
    "            ayats[index]['AYAT_NUMBER'] = i\n",
    "            \n",
    "            words = {\n",
    "                'OPEN TAG' : '',\n",
    "                'AYAT' : int(row['AYAT_NUMBER']),\n",
    "                'ARAB' : row['ARAB'],\n",
    "                'TAG' : row['TAG'],\n",
    "                'WORD_NUMBER' : row['WORD_NUMBER'],\n",
    "                'AYAT_NUMBER' : row['AYAT_NUMBER'],\n",
    "                'CLOSE TAG' : ''\n",
    "            }\n",
    "            \n",
    "            grouppedAyats[i].append(words)\n",
    "\n",
    "        index += 1\n",
    "        \n",
    "    return grouppedAyats\n",
    "\n",
    "def ungroup(ayats):\n",
    "\n",
    "    ungrouppedAyats = []\n",
    "\n",
    "    for ayat in ayats:\n",
    "        for word in ayat:\n",
    "            ungrouppedAyats.append(word)\n",
    "\n",
    "    return ungrouppedAyats\n",
    "\n",
    "def classify(patterns, ayats):\n",
    "\n",
    "    # Prepare data\n",
    "    ayats = group(ayats)\n",
    "\n",
    "    # Rule based logic\n",
    "    ayatIndex = 0\n",
    "    entityIndex = 0\n",
    "\n",
    "    for ayat in ayats:\n",
    "        for pattern in patterns:\n",
    "            patternLength = len(pattern)\n",
    "            for wordIndex in range(len(ayat) - patternLength):\n",
    "                # Get Array of words based on the pattern length\n",
    "                wordTagsArray = []\n",
    "                for wordOfArrayIndex in range(patternLength):\n",
    "                    wordTagsArray.append(ayat[wordIndex+wordOfArrayIndex]['TAG'])\n",
    "                if wordTagsArray == pattern:\n",
    "                    print('foundd')\n",
    "                    ayat[wordIndex]['OPEN TAG'] += str(entityIndex) + '('\n",
    "                    ayat[wordIndex+patternLength]['CLOSE TAG'] += str(entityIndex) + ')'\n",
    "                    entityIndex += 1\n",
    "        ayatIndex += 1\n",
    "\n",
    "    return ungroup(ayats)\n",
    "\n",
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
